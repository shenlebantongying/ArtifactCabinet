\section{Random variables}

\subsection{Basis of random variables}

\begin{fact}{Random variable definition}{}
  A random variable $X$ is a function $X:\samplespace\rightarrow\mathbb{R}$ that maps an outcome $o\in\samplespace$ to a number $X(o)$ on the real line.

  Note that $\Omega$ is the sample space.
\end{fact}

\begin{fact}{PMF \g Probability mass function}{}
    The PMF of a discrete random variable $X$ is the function $p_X$ given by $p_X(x) = \mathbb{P}(X=x)$.

    Note that $X=x$ denote an event, consisting of all outcomes to which $X$ assigns the number $x$.
\end{fact}

\begin{fact}{PDF \g Probability density function}{}
  Let $X$ be a continuous random variable.

  The PDF of X is a function $f_X\mathbin{:}\Omega\rightarrow\mathbb{R}$ that, when integrated
  over an interval $[a,v]$, yields the probability of obtaining $a\le X \le b$:
  \begin{equation*}
    \mathbb{P}[a\le X \le b] = \int_{a}^{v}f(x)dx.
  \end{equation*}
  Basic properties:
  \begin{itemize}
    \item Unity: $\int_\samplespace f(x)dx=1$ (Integration over the entire sample space of PDF yields 1.)
  \end{itemize}
\end{fact}

\begin{fact}{CDF \g Cumulative distribution functions}{}
    Let $X$ be a continuous random variable with sample space $\samplespace=\mathbb{R}$.
    The CDF of $X$ is
    \begin{equation*}
      F_X(x)=\mathbb{P}(X\le x)=\int_{-\infty}^{x}f_X(t)dt.
    \end{equation*}
\end{fact}

\begin{fact}{Expectation definition}{}
    The \textbf{expectation} of a continuous random variable X is
    \begin{equation*}
        \mathbb{E}[X] = \int_{\samplespace}xf_X(x)\,dx.
    \end{equation*}
    The \textbf{expectation} of a discrete random variable X is
    \begin{equation*}
        \mathbb{E}[X]=\sum_{x\in \samplespace}x f_X(x).
    \end{equation*}
\end{fact}

\begin{fact}{Variance definition}{}

    The \textbf{variance} of a continuous random variable X is
    \begin{equation*}
        \Var[X] = \mathbb{E}[(X-\mu)^2]=\int_{\samplespace}(x-\mu)^2f_X(x)\,dx
    \end{equation*}
    where $\mu=\mathbb{E}[X]$ is the expectation of $X$.
\end{fact}

Note that if a function $g$ is applied to random variable $X$, the expectation can be found via
\begin{equation*}
  \mathbb{E}[g(X)]=\int_{\samplespace}g(x)f_X(x)\,dx.
\end{equation*}

\subsection{Bernoulli}

\begin{fact}{Bernoulli random variable}{}
    The expression
    \begin{equation*}
        X \sim Bernoulli(p)
    \end{equation*}
    means $X$ is drawn form a Bernoulli distribution with parameter $p$
    , which is controls the probability of obtaining 1.

    The PMF of $X$ is
    \begin{align*}
        p_X(0) &= 1-p\\
        p_X(1) &= p
    \end{align*}
    where $0<p<1$.
\end{fact}

\subsection{Binomial}

\begin{fact}{Binomial random variable}{}
  For $X \sim Binomial(n,p)$, the PMF of X is
  \begin{align*}
    f_X(k) = \binom{n}{k}p^k(1-p)^{n-k} &&k=0,1,...
  \end{align*}
  where $p$ is the binomial parameter, and n is the total number of stats.
\end{fact}

\subsection{Poisson}
A special case of Binomial distribution, where $p\rightarrow0,n\rightarrow\infty$.

\begin{fact}{Poisson random variable}{}
  Random variable $X$ is drawn from a Poisson distribution with a parameter $\lambda$.
  \begin{equation*}
      X \sim \text{Poisson}(\lambda)
  \end{equation*}

  PMF of X is
  \begin{align*}
    f_X(k) =  \frac{\lambda^k}{k!}e^{-\lambda} && k = 0,1,...
  \end{align*}
  Variable $k$ is the number of events that occurs in a range of time or a region of space (or whatever).

  The parameter $\lambda$ determines the rate of the arrival or occurrence.
\end{fact}

\begin{exec}
  Show that if $X\sim \text{Poisson}(\lambda)$, then $\mathbb{E}[X]=\lambda$.
  \tcblower
  \begin{align*}
    \mathbb{E}[X]
    &= \sum_{k=0}^{\infty} k\frac{\lambda^k}{k!}e^{-\lambda}\\
    &= \sum_{k=1}^{\infty}\frac{\lambda^k}{(k-1)!}e^{-\lambda} = \lambda e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}\\
    &= \lambda e^{-\lambda}\sum_{r=0}^{\infty}\frac{\lambda^{r}}{r!} = \lambda e^{-\lambda}e^{\lambda}\\
    &= \lambda.
  \end{align*}
  Note that $e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$ (The Maclaurin series of $e^x$).
\end{exec}

\subsection{Gaussian}

\begin{fact}{Gaussian random variable}{}
  Let $X$ be a Gaussian (Normal) random variable.
  \begin{equation*}
      X \sim \mathcal{N}(\mu,\sigma^2)
  \end{equation*}
  Its PMF of is
  \begin{equation*}
    f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right],
  \end{equation*}
  where $(\mu\rightarrow mean,\sigma^2\rightarrow variance)$ are parameters of the distribution.

  It can also be rewritten by letting $b=\frac{1}{2\sigma^2}$, and thus
  \begin{equation*}
    f(x)=\sqrt{\frac{b}{\pi}}e^{-b(x-\mu)^2}.
  \end{equation*}

  Its expectation $\mathbb{E}[X]=\mu$ and variance $\Var[X]=\sigma^2$.

  \textbf{Standard Gaussian} random variable is a Gaussian with $\mu=0$ and $\sigma^2=1$. Its PDF is
  \begin{equation*}
    f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
  \end{equation*}
\end{fact}

\begin{exec}
  Using multivariate Standard Gaussian distribution to obtain random points on a sphere.
  \tcblower
  Suppose that $x,y,z$ are 3 independent random variables that follows Standard Gaussian distribution.
  The probability of $x,y,z$ are certain values $(x',y',z')$ is
  \begin{align*}
    f(x')f(y')f(z')=\frac{1}{\sqrt{2\pi}}e^{-\frac{x'^2+y'^2+z'^2}{2}}.
  \end{align*}
  Normalizing $(x',y',z')$ by multiplying each with $r'=\frac{1}{\sqrt{x'^2+y'^2+z'^2}}$ will yield a point $(\frac{x'}{r'},\frac{y'}{r'},\frac{z'}{r'})$ at a unit sphere.
\end{exec}

\begin{fact}{CDF of standard Gaussian and Error function}{}
  For standard Gaussian, the CDF is defined as
  \begin{equation*}
    \Phi(x)=\int_{-\infty}^{x}f(t)dt=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x} e^{-\frac{t^2}{2}}\,dt.
  \end{equation*}
  The \textbf{error function} is defined as
  \begin{equation*}
    \Erf (x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^2}\,dt.
  \end{equation*}
\end{fact}

\begin{exec}
  Show that $\Erf(x)=2\Phi(x\sqrt{2})-1$
  \tcblower
  \TODO
  \begin{align*}
    2\Phi(x\sqrt{2})-1
    &= 2\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x\sqrt{2}}e^{-\frac{t^2}{2}\,dt}-1\\
  \end{align*}
  \TODO hint: polar coordinate and E
  \begin{equation*}
    \int e^{-x^2}\,dt = \int_{0}^{x} e^{t^2}\,dx = \frac{\sqrt{\pi}}{2}\Erf(x)
  \end{equation*}
\end{exec}

\begin{fact}{CDF of arbitrary Gaussian}{}
  Let $X\sim \mathcal{N}(\mu, \sigma^2)$
  \begin{equation*}
    F_X(x)=\Phi(\frac{x-\mu}{\sigma}) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\frac{x-\mu}{\sigma}} e^{-\frac{t^2}{2}}\,dt.
  \end{equation*}
\end{fact}

\begin{fact}{Convolution}{}
  The convolution of two function $f$ and $g$ for a value $x$ over infinite range $t$ is given by
  \begin{equation*}
    (f*g)[x] = \int_{-\infty}^{\infty}f(t)g(x-t)\,dt.
  \end{equation*}
\end{fact}

For two random variables $X$ and $Y$, the PDF of $X+Y$ is equivalent to convolving the PDFs of $X$ and $Y$.
